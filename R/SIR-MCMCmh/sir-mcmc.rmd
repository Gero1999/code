---
title: "SIR Model and MCMC-MH to solve it"
author: "Gerardo José Rodríguez"
date: "2023-10-09"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
```


## SIR MODEL

```{r}
# Define the SIR model function
sir_model <- function(t, beta, gamma, S0, I0) {
  dS <- -beta * S0 * I0
  dI <- beta * S0 * I0 - gamma * I0
  dR <- gamma * I0
  return(list(c(dS, dI, dR)))
}

# Load your time series data into variables (e.g., time, observed S, I, R)

# Fit the SIR model to the data
fit <- nls(
  c(S, I, R) ~ sir_model(time, beta, gamma, S0, I0),
  start = list(beta = initial_guess_beta, gamma = initial_guess_gamma, S0 = S[1], I0 = I[1])
)

# Extract parameter estimates
estimates <- coef(fit)
```


```{r}
# Load the MCMCpack library
library(MCMCpack)

# Define the likelihood function
likelihood <- function(params) {
  beta <- params[1]
  gamma <- params[2]
  S0 <- params[3]
  I0 <- params[4]
  # Simulate the SIR model and compute likelihood
  sim <- ode(y = c(S0, I0, 1 - S0 - I0), times = time_points, func = sir_model, parms = c(beta, gamma))
  predicted_cases <- sim[, "I"]
  log_likelihood <- sum(dpois(observed_cases, lambda = predicted_cases, log = TRUE))
  return(log_likelihood)
}

# Specify the Metropolis-Hastings MCMC settings
metropolis_settings <- MCMCmetrop1R(fn = likelihood, p = 4, 
                                     burnin = 1000, mcmc = 5000, 
                                     thin = 1, initial = initial_guess_params, 
                                     scale = c(0.1, 0.1, 0.1, 0.1))

# Run the Metropolis-Hastings algorithm
results <- MCMCmetrop1R(metropolis_settings)

# Extract parameter samples from the MCMC output
parameter_samples <- results[, 1:4]
```


## MCMC Metropolis Hasting algorithm

```{r}
lg = function(mu, n, ybar) {
  mu2 = mu^2
  n * (ybar * mu - mu2 / 2.0) - log(1 + mu2)
}

mh = function(n, ybar, n_iter, mu_init, cand_sd) {
  ## Random-Walk Metropolis-Hastings algorithm
  
  ## step 1, initialize
  mu_out = numeric(n_iter)
  accpt = 0
  mu_now = mu_init
  lg_now = lg(mu=mu_now, n=n, ybar=ybar)
  
  ## step 2, iterate
  for (i in 1:n_iter) {
    ## step 2a
    mu_cand = rnorm(n=1, mean=mu_now, sd=cand_sd) # draw a candidate
    
    ## step 2b
    lg_cand = lg(mu=mu_cand, n=n, ybar=ybar) # evaluate log of g with the candidate
    lalpha = lg_cand - lg_now # log of acceptance ratio
    alpha = exp(lalpha)
    
    ## step 2c
    u = runif(1) # draw a uniform variable which will be less than alpha with probability min(1, alpha)
    if (u < alpha) { # then accept the candidate
      mu_now = mu_cand
      accpt = accpt + 1 # to keep track of acceptance
      lg_now = lg_cand
    }
    
    ## collect results
    mu_out[i] = mu_now # save this iteration's value of mu
  }
  
  ## return a list of output
  list(mu=mu_out, accpt=accpt/n_iter)
}
```


```{r}
y = c(1.2, 1.4, -0.5, 0.3, 0.9, 2.3, 1.0, 0.1, 1.3, 1.9)
ybar = mean(y)
n = length(y)
hist(y, freq=FALSE, xlim=c(-1.0, 3.0)) # histogram of the data
curve(dt(x=x, df=1), lty=2, add=TRUE) # prior for mu
points(y, rep(0,n), pch=1) # individual data points
points(ybar, 0, pch=19) # sample mean
```


```{r}
set.seed(43) # set the random seed for reproducibility
post = mh(n=n, ybar=ybar, n_iter=1e3, mu_init=0.0, cand_sd=3.0)
str(post)
post$accpt
```

```{r}
library("coda")
traceplot(as.mcmc(post$mu))
```

## MCMC-MH Using JAGS
```{r}
library('rjags')

# Specify the model
mod_string = "model{
  for (i in 1:n){
    y[i] ~ dnorm(mu, 1.0/sig2) 
  }
  mu ~ dt(0.0, 1.0/1.0, 1)
  sig2 = 1.0
}"

# Set up of the model
y = c(1.2,1.4,-0.5,0.3,0.9,2.3,1.0,0.1)
n=length(y)

data_jags = list(y=y, n=n)
params=c('mu')

inits = function(){
  inits=list("mu"=0.0)
}

mod = jags.mode(textConnection(mod_string), data=data_jags, inits=inits)

# Run the MCMC Sampler
update(mod, 500)
mod_sim = coda.samples(model=mod, variable.names=params, n.iter=1000)

# Post processing
library("coda")
plot(mod_sim)
summary(mod_sim)
```

